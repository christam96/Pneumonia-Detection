{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 files belonging to 2 classes.\n",
      "Found 16 files belonging to 2 classes.\n",
      "Found 624 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_dir = 'chest_xray/chest_xray/train'\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  training_dir,\n",
    "  seed=123,\n",
    "  image_size=(200,200),\n",
    "  batch_size=4)\n",
    "\n",
    "validation_dir = 'chest_xray/chest_xray/val'\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  validation_dir,\n",
    "  seed=123,\n",
    "  image_size=(200,200),\n",
    "  batch_size=4)\n",
    "\n",
    "testing_dir = 'chest_xray/chest_xray/test'\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  testing_dir,\n",
    "  seed=123,\n",
    "  image_size=(200,200),\n",
    "  batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 200, 200, 3)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate the normalization layer\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure dataset for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_tensors = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_tensors = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1304/1304 [==============================] - 266s 204ms/step - loss: 0.2109 - accuracy: 0.9273 - val_loss: 0.2813 - val_accuracy: 0.8750\n",
      "Epoch 2/10\n",
      "1304/1304 [==============================] - 263s 201ms/step - loss: 0.1124 - accuracy: 0.9607 - val_loss: 0.3062 - val_accuracy: 0.9375\n",
      "Epoch 3/10\n",
      "1304/1304 [==============================] - 260s 199ms/step - loss: 0.0573 - accuracy: 0.9804 - val_loss: 0.3465 - val_accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "1057/1304 [=======================>......] - ETA: 49s - loss: 0.0480 - accuracy: 0.9818"
     ]
    }
   ],
   "source": [
    "num_classes = 1\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "    loss=tf.losses.BinaryCrossentropy(from_logits=False),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=NUM_EPOCHS\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['accuracy', 'loss']):\n",
    "    ax[i].plot(history.history[met])\n",
    "    ax[i].plot(history.history['val_' + met])\n",
    "    ax[i].set_title('Model {}'.format(met))\n",
    "    ax[i].set_xlabel('epochs')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "import cv2\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# I will be making predictions off of the test set in one batch size\n",
    "# This is useful to be able to get the confusion matrix\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "input_path = 'chest_xray/chest_xray/'\n",
    "\n",
    "for cond in ['/NORMAL/', '/PNEUMONIA/']:\n",
    "    for img in (os.listdir(input_path + 'test' + cond)):\n",
    "        img = plt.imread(input_path+'test'+cond+img)\n",
    "        img = cv2.resize(img, (200, 200))\n",
    "        img = np.dstack([img, img, img])\n",
    "        img = img.astype('float32') / 255\n",
    "        if cond=='/NORMAL/':\n",
    "            label = 0\n",
    "        elif cond=='/PNEUMONIA/':\n",
    "            label = 1\n",
    "        test_data.append(img)\n",
    "        test_labels.append(label)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "preds = model.predict(test_ds)\n",
    "\n",
    "acc = accuracy_score(test_labels, np.round(preds))*100\n",
    "cm = confusion_matrix(test_labels, np.round(preds))\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print('CONFUSION MATRIX ------------------')\n",
    "print(cm)\n",
    "\n",
    "print('\\nTEST METRICS ----------------------')\n",
    "precision = tp/(tp+fp)*100\n",
    "recall = tp/(tp+fn)*100\n",
    "print('Accuracy: {}%'.format(acc))\n",
    "print('Precision: {}%'.format(precision))\n",
    "print('Recall: {}%'.format(recall))\n",
    "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))\n",
    "\n",
    "print('\\nTRAIN METRIC ----------------------')\n",
    "print('Train acc: {}'.format(np.round((history.history['accuracy'][-1])*100, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "* [ ] Change batch size\n",
    "* [ ] Implement drop out\n",
    "* [ ] Explore other model architectures\n",
    "    * [ ] Convolutional NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Batch size should be large enough so that each batch has enough of a chance to see enough of each label.\n",
    "###### Remember that there are 3 times as many 'PNEUMONIA' labels as there are 'NORMAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "training_dir = 'chest_xray/chest_xray/train'\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  training_dir,\n",
    "  seed=123,\n",
    "  image_size=(200,200),\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_dir = 'chest_xray/chest_xray/val'\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  validation_dir,\n",
    "  seed=123,\n",
    "  image_size=(200,200),\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "testing_dir = 'chest_xray/chest_xray/test'\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  testing_dir,\n",
    "  seed=123,\n",
    "  image_size=(200,200),\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "# Instatiate the normalization layer\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_tensors = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_tensors = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "num_classes = 1\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "    loss=tf.losses.BinaryCrossentropy(from_logits=False),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=NUM_EPOCHS\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['accuracy', 'loss']):\n",
    "    ax[i].plot(history.history[met])\n",
    "    ax[i].plot(history.history['val_' + met])\n",
    "    ax[i].set_title('Model {}'.format(met))\n",
    "    ax[i].set_xlabel('epochs')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "import cv2\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# I will be making predictions off of the test set in one batch size\n",
    "# This is useful to be able to get the confusion matrix\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "input_path = 'chest_xray/chest_xray/'\n",
    "\n",
    "for cond in ['/NORMAL/', '/PNEUMONIA/']:\n",
    "    for img in (os.listdir(input_path + 'test' + cond)):\n",
    "        img = plt.imread(input_path+'test'+cond+img)\n",
    "        img = cv2.resize(img, (200, 200))\n",
    "        img = np.dstack([img, img, img])\n",
    "        img = img.astype('float32') / 255\n",
    "        if cond=='/NORMAL/':\n",
    "            label = 0\n",
    "        elif cond=='/PNEUMONIA/':\n",
    "            label = 1\n",
    "        test_data.append(img)\n",
    "        test_labels.append(label)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "preds = model.predict(test_ds)\n",
    "\n",
    "acc = accuracy_score(test_labels, np.round(preds))*100\n",
    "cm = confusion_matrix(test_labels, np.round(preds))\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print('CONFUSION MATRIX ------------------')\n",
    "print(cm)\n",
    "\n",
    "print('\\nTEST METRICS ----------------------')\n",
    "precision = tp/(tp+fp)*100\n",
    "recall = tp/(tp+fn)*100\n",
    "print('Accuracy: {}%'.format(acc))\n",
    "print('Precision: {}%'.format(precision))\n",
    "print('Recall: {}%'.format(recall))\n",
    "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))\n",
    "\n",
    "print('\\nTRAIN METRIC ----------------------')\n",
    "print('Train acc: {}'.format(np.round((history.history['accuracy'][-1])*100, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "training_dir = 'chest_xray/chest_xray/train'\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  training_dir,\n",
    "  seed=123,\n",
    "  image_size=(200,200),\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_dir = 'chest_xray/chest_xray/val'\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  validation_dir,\n",
    "  seed=123,\n",
    "  image_size=(200,200),\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "# Instatiate the normalization layer\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_tensors = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_tensors = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "  layers.Dropout(rate=0.2),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(rate=0.5),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(rate=0.5),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "    loss=tf.losses.BinaryCrossentropy(from_logits=False),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=NUM_EPOCHS\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['accuracy', 'loss']):\n",
    "    ax[i].plot(history.history[met])\n",
    "    ax[i].plot(history.history['val_' + met])\n",
    "    ax[i].set_title('Model {}'.format(met))\n",
    "    ax[i].set_xlabel('epochs')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "import cv2\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# I will be making predictions off of the test set in one batch size\n",
    "# This is useful to be able to get the confusion matrix\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "input_path = 'chest_xray/chest_xray/'\n",
    "\n",
    "for cond in ['/NORMAL/', '/PNEUMONIA/']:\n",
    "    for img in (os.listdir(input_path + 'test' + cond)):\n",
    "        img = plt.imread(input_path+'test'+cond+img)\n",
    "        img = cv2.resize(img, (200, 200))\n",
    "        img = np.dstack([img, img, img])\n",
    "        img = img.astype('float32') / 255\n",
    "        if cond=='/NORMAL/':\n",
    "            label = 0\n",
    "        elif cond=='/PNEUMONIA/':\n",
    "            label = 1\n",
    "        test_data.append(img)\n",
    "        test_labels.append(label)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "preds = model.predict(test_ds)\n",
    "\n",
    "acc = accuracy_score(test_labels, np.round(preds))*100\n",
    "cm = confusion_matrix(test_labels, np.round(preds))\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print('CONFUSION MATRIX ------------------')\n",
    "print(cm)\n",
    "\n",
    "print('\\nTEST METRICS ----------------------')\n",
    "precision = tp/(tp+fp)*100\n",
    "recall = tp/(tp+fn)*100\n",
    "print('Accuracy: {}%'.format(acc))\n",
    "print('Precision: {}%'.format(precision))\n",
    "print('Recall: {}%'.format(recall))\n",
    "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))\n",
    "\n",
    "print('\\nTRAIN METRIC ----------------------')\n",
    "print('Train acc: {}'.format(np.round((history.history['accuracy'][-1])*100, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Conv2D Layers (No Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "training_dir = 'chest_xray/chest_xray/train'\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  training_dir,\n",
    "  seed=123,\n",
    "  image_size=(200,200),\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_dir = 'chest_xray/chest_xray/val'\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  validation_dir,\n",
    "  seed=123,\n",
    "  image_size=(200,200),\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "# Instatiate the normalization layer\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_tensors = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_tensors = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "  \n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  \n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  \n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  \n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "    loss=tf.losses.BinaryCrossentropy(from_logits=False),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=NUM_EPOCHS\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['accuracy', 'loss']):\n",
    "    ax[i].plot(history.history[met])\n",
    "    ax[i].plot(history.history['val_' + met])\n",
    "    ax[i].set_title('Model {}'.format(met))\n",
    "    ax[i].set_xlabel('epochs')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "import cv2\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# I will be making predictions off of the test set in one batch size\n",
    "# This is useful to be able to get the confusion matrix\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "input_path = 'chest_xray/chest_xray/'\n",
    "\n",
    "for cond in ['/NORMAL/', '/PNEUMONIA/']:\n",
    "    for img in (os.listdir(input_path + 'test' + cond)):\n",
    "        img = plt.imread(input_path+'test'+cond+img)\n",
    "        img = cv2.resize(img, (200, 200))\n",
    "        img = np.dstack([img, img, img])\n",
    "        img = img.astype('float32') / 255\n",
    "        if cond=='/NORMAL/':\n",
    "            label = 0\n",
    "        elif cond=='/PNEUMONIA/':\n",
    "            label = 1\n",
    "        test_data.append(img)\n",
    "        test_labels.append(label)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "preds = model.predict(test_ds)\n",
    "\n",
    "acc = accuracy_score(test_labels, np.round(preds))*100\n",
    "cm = confusion_matrix(test_labels, np.round(preds))\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print('CONFUSION MATRIX ------------------')\n",
    "print(cm)\n",
    "\n",
    "print('\\nTEST METRICS ----------------------')\n",
    "precision = tp/(tp+fp)*100\n",
    "recall = tp/(tp+fn)*100\n",
    "print('Accuracy: {}%'.format(acc))\n",
    "print('Precision: {}%'.format(precision))\n",
    "print('Recall: {}%'.format(recall))\n",
    "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))\n",
    "\n",
    "print('\\nTRAIN METRIC ----------------------')\n",
    "print('Train acc: {}'.format(np.round((history.history['accuracy'][-1])*100, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# I will be making predictions off of the test set in one batch size\n",
    "# This is useful to be able to get the confusion matrix\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "input_path = 'chest_xray/chest_xray/'\n",
    "\n",
    "for cond in ['/NORMAL/', '/PNEUMONIA/']:\n",
    "    for img in (os.listdir(input_path + 'test' + cond)):\n",
    "        img = plt.imread(input_path+'test'+cond+img)\n",
    "        img = cv2.resize(img, (200, 200))\n",
    "        img = np.dstack([img, img, img])\n",
    "        img = img.astype('float32') / 255\n",
    "        if cond=='/NORMAL/':\n",
    "            label = 0\n",
    "        elif cond=='/PNEUMONIA/':\n",
    "            label = 1\n",
    "        test_data.append(img)\n",
    "        test_labels.append(label)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "preds = model.predict(test_ds)\n",
    "\n",
    "acc = accuracy_score(test_labels, np.round(preds))*100\n",
    "cm = confusion_matrix(test_labels, np.round(preds))\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print('CONFUSION MATRIX ------------------')\n",
    "print(cm)\n",
    "\n",
    "print('\\nTEST METRICS ----------------------')\n",
    "precision = tp/(tp+fp)*100\n",
    "recall = tp/(tp+fn)*100\n",
    "print('Accuracy: {}%'.format(acc))\n",
    "print('Precision: {}%'.format(precision))\n",
    "print('Recall: {}%'.format(recall))\n",
    "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))\n",
    "\n",
    "print('\\nTRAIN METRIC ----------------------')\n",
    "print('Train acc: {}'.format(np.round((history.history['accuracy'][-1])*100, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
